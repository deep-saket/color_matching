# ğŸ¨ Color Matching Engine for Hair Swatch Recommendation

This repository provides a modular pipeline to analyze a user's hair from portrait images and recommend the most similar hair swatch using either classical computer vision (CV) or vision-language models (VLMs) like Qwen2.5-VL or ColPali.

---

## ğŸ“ Repository Structure

```
color_matching/
â”œâ”€â”€ common/                  # Base interfaces for inference components
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ loader/              # Configuration loader (settings.yml)
â”‚   â””â”€â”€ files/               # Optional swatch files or templates
â”œâ”€â”€ models/                 # Pre-built inference models (hair segmenter, VLMs)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ HairMatchGeneratorCV.py     # Classic CV pipeline
â”‚   â”œâ”€â”€ SwatchMatchGenerator.py     # VLM-based swatch matcher
â”‚   â””â”€â”€ helpers/                    # Feature matchers and utilities
â”œâ”€â”€ local_test.py           # Entrypoint script to run a full inference
â”œâ”€â”€ local_test_params.yml   # Local test configuration file
â”œâ”€â”€ setup_env.sh            # One-click dependency installer
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation


> âš ï¸ Before running anything, ensure you execute the setup script:
> ```bash
> bash setup_env.sh
> ```
```bash
git clone https://github.com/your-org/color_matching.git
cd color_matching
bash setup_env.sh
```

---

## ğŸ§ª Running a Test

Use the built-in test runner:

```bash
python local_test.py
```

This script loads configuration from `local_test_params.yml` and `settings.yml`.

---

## ğŸ› ï¸ Configuration Guide

### ğŸ”§ `config/loader/settings.yml`

This YAML controls system-level configuration and model settings.

```yaml
general:
  artefacts_dir: ./artefacts     # Where input/mask/results will be saved

hair_match_generator:
  args:
    swatch_path: ./swatches       # Folder containing swatch images
```

If you're using VLMs instead of CV:

```yaml
swatch_match_generator:
  args:
    swatch_path: ./swatches
    device: cpu                   # or mps / cuda
    vlm_candidate: QwenV25Infer   # Model class name from models/
    models: [QwenV25Infer]        # List of model classes to load
```

---

### ğŸ“„ `local_test_params.yml`

This file tells the test script what image and pipeline to use:

```yaml
run_mode: cv     # or vlm

input_image: ./dataset/portraits/sample1.jpg

generator:
  name: HairMatchGeneratorCV      # or SwatchMatchGenerator for VLM
```

---

## ğŸ“‚ Dataset Organization

Swatches should be stored as cropped square color patches:

```
./swatches/
â”œâ”€â”€ black.png
â”œâ”€â”€ light_blonde.jpg
â”œâ”€â”€ golden_brown.jpeg
```

Portraits (user-uploaded inputs):

```
./dataset/portraits/
â”œâ”€â”€ sample1.jpg
â”œâ”€â”€ user_test.png
```

Make sure these are all front-facing and well-lit, ideally cropped to head-and-shoulder shots.

---

## ğŸ§© Core Components

### `HairMatchGeneratorCV`

Classical pipeline:
- Uses `HairSegmenter` to extract hair mask
- Matches to `HairSwatchMatcherCV` using LAB color stats

**Artifacts saved to**: `./artefacts/HairMatchGeneratorCV/`

### `SwatchMatchGenerator`

VLM-based approach:
- Uses ColPali or Qwen2.5-VL to extract embeddings
- Matches to swatches using text/image embedding similarity

**Artifacts saved to**: `./artefacts/SwatchMatchGenerator/`

---

## ğŸ§  Customization

- Add new matchers in `src/`
- Add new models in `models/`, register in `ModelManager`
- Extend segmentation logic in `HairSegmenter.py`
- Preprocess portrait images in `local_test.py`

---

## ğŸ§¾ Outputs

Each run saves:

```
artefacts/
â””â”€â”€ HairMatchGeneratorCV/
    â”œâ”€â”€ HairMatchGeneratorCV_<timestamp>_input.png
    â”œâ”€â”€ HairMatchGeneratorCV_<timestamp>_hair_mask.png
    â””â”€â”€ [optional future] result.json
```

---

## ğŸ“œ License

MIT License
